# -*- coding: utf-8 -*-
"""UAS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AioeIdnGhDJb_On0VXBEdZXeWvHQr54O

UAS BIG DATA

FARIS AMMAR FAISHAL
22.11.5081

ATQIYA TRIANDA PUTRA A.
22.11.5083
"""

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, isnan, when, count, mean, corr
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.sql.types import IntegerType, DoubleType
import pandas as pd

spark = SparkSession.builder \
    .appName("Student Performance Analysis") \
    .config("spark.driver.memory", "2g") \
    .getOrCreate()

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/Big Data Lanjut/StudentPerformanceFactors.csv'
data = spark.read.csv(file_path, header=True, inferSchema=True)

"""LANGKAH C"""

data.printSchema()

data.show(5)

print("\nNull Value Counts:")
null_counts = data.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in data.columns])
null_counts.show()

# Define column mappings (old_name -> new_name)
column_mappings = {
    "Hours_Studied": "study_hours",
    "Attendance": "attendance",
    "Parental_Involvement": "parental_guidance",
    "Access_to_Resources": "resources_access",
    "Extracurricular_Activities": "extracurricular",
    "Sleep_Hours": "sleep_hours",
    "Previous_Scores": "prev_score",
    "Motivation_Level": "motivation",
    "Internet_Access": "internet_access",
    "Tutoring_Sessions": "tutoring_sessions",
    "Family_Income": "family_income",
    "Teacher_Quality": "teacher_quality",
    "School_Type": "school",
    "Peer_Influence": "peer_influence",
    "Physical_Activity": "physical_activity",
    "Learning_Disabilities": "learning_disabilities",
    "Parental_Education_Level": "parent_last_education",
    "Distance_from_Home": "home_distance",
    "Gender": "gender",
    "Exam_Score": "current_score"
}

#  Identify categorical and numerical columns with new names
categorical_columns = [
    "parental_guidance", "resources_access", "extracurricular", "motivation", "internet_access",
    "family_income", "teacher_quality", "school", "peer_influence", "learning_disabilities",
    "parent_last_education", "home_distance", "gender"
]

numeric_columns = [
    "study_hours", "attendance", "sleep_hours", "prev_score",
    "tutoring_sessions", "physical_activity", "current_score"
]

renamed_data = data
for old_col, new_col in column_mappings.items():
    renamed_data = renamed_data.withColumnRenamed(old_col, new_col)

print("Null values before imputation:")
null_counts = renamed_data.select([count(when(col(c).isNull() | isnan(c), c)).alias(c)
                                 for c in renamed_data.columns])
null_counts.show()

total_rows = data.count()
print("\nPercentage of missing values:")
for column in data.columns:
    missing_count = data.filter(col(column).isNull()).count()
    if missing_count > 0:
        percentage = (missing_count / total_rows) * 100
        print(f"{column}: {missing_count} rows ({percentage:.2f}%)")

processed_data = renamed_data

for cat_col in categorical_columns:
    # Calculate mode
    mode_df = renamed_data.groupBy(cat_col).count().orderBy("count", ascending=False)
    mode_value = mode_df.first()[0]
    # Fill nulls with mode
    processed_data = processed_data.fillna(value=mode_value, subset=[cat_col])

# Create and apply StringIndexer for categorical columns
for column in categorical_columns:
    # Create temporary column name
    temp_col = f"{column}_temp"

    # Create and apply indexer
    indexer = StringIndexer(inputCol=column, outputCol=temp_col, handleInvalid="keep")
    processed_data = indexer.fit(processed_data).transform(processed_data)

    # Drop original and rename temp column
    processed_data = processed_data.drop(column).withColumnRenamed(temp_col, column)

#  Verify schema
print("\nFinal Schema:")
processed_data.printSchema()

# Show sample of processed data
print("\nSample of processed data:")
processed_data.show(5)

# Calculate correlation matrix
print("\nCalculating correlation matrix...")
numeric_data = processed_data.toPandas()
correlation_matrix = numeric_data.corr()

# Create correlation heatmap
plt.figure(figsize=(15, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix of All Features')
plt.tight_layout()
plt.show()

# Cache the preprocessed data
processed_data.cache()

print("\nPreprocessing completed successfully!")

# Exploratory Data Analysis (EDA)

# 1. Bar Chart - Parental Education Level Distribution
parent_edu_dist = processed_data.groupBy('parent_last_education').count().orderBy('count', ascending=False).toPandas()
plt.figure(figsize=(12, 6))
plt.bar(parent_edu_dist.index, parent_edu_dist['count'], color='skyblue', edgecolor='black')
plt.title('Distribution of Parental Education Levels')
plt.xlabel('Education Level')
plt.ylabel('Number of Students')
plt.xticks(parent_edu_dist.index, ['High School', 'College', 'Postgraduate',], rotation=45)
plt.tight_layout()
plt.show()

# 2. Pie Chart - Extracurricular Activities Distribution
extra_dist = processed_data.groupBy('extracurricular').count().toPandas()
plt.figure(figsize=(8, 8))
plt.pie(extra_dist['count'],
        labels=['With Extracurricular', 'Without Extracurricular'],
        autopct='%1.1f%%',
        colors=['lightblue', 'lightgreen'])
plt.title('Distribution of Students with/without Extracurricular Activities')
plt.show()

# 3. Box Plot - Exam Scores by Motivation Level
plt.figure(figsize=(10, 6))
df_pandas = processed_data.toPandas()
sns.boxplot(data=df_pandas, x='motivation', y='current_score')
plt.title('Exam Scores Distribution by Motivation Level', pad=20)
plt.xlabel('Motivation Level', labelpad=10)
plt.ylabel('Exam Score', labelpad=10)

# Add value labels for median lines
medians = df_pandas.groupby('motivation')['current_score'].median()
for idx, median in enumerate(medians):
    plt.text(idx, median, f'Median: {median:.1f}',
             horizontalalignment='center',
             verticalalignment='bottom',
             color='black',
             fontweight='bold')

# Customize x-axis labels
plt.xticks([0, 1, 2], ['Low', 'Medium', 'High'])

# Add a grid for better readability
plt.grid(True, axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show

# 4. Scatter Plot - Study Hours vs Exam Score
plt.figure(figsize=(10, 6))
plt.scatter(processed_data.select('study_hours').toPandas(),
           processed_data.select('current_score').toPandas())
plt.title('Study Hours vs Exam Score')
plt.xlabel('Study Hours')
plt.ylabel('Exam Score')
plt.show()

# Feature Selection based on correlation with target variable
feature_correlations = []
target_col = 'current_score'
CORRELATION_THRESHOLD = 0.1  # Selecting features with correlation > 0.1

for column in processed_data.columns:
    if column != target_col:
        correlation = processed_data.stat.corr(column, target_col)
        feature_correlations.append((column, abs(correlation)))

# Sort features by absolute correlation
sorted_features = sorted(feature_correlations, key=lambda x: x[1], reverse=True)
print("\nFeature Correlations with Exam Score:")
for feature, correlation in sorted_features:
    print(f"{feature}: {correlation:.3f}")

# Select top features based on correlation
selected_features = [feature[0] for feature in sorted_features if feature[1] > CORRELATION_THRESHOLD]
print(f"\nSelected Features (correlation > {CORRELATION_THRESHOLD}):", selected_features)
print(f"Number of selected features: {len(selected_features)}")

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, DecisionTreeClassifier, MultilayerPerceptronClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.sql.functions import col, when
import matplotlib.pyplot as plt
import numpy as np

# Prepare the feature vector
selected_features = ['attendance', 'study_hours', 'prev_score', 'tutoring_sessions',
                    'parent_last_education', 'peer_influence']

# Create feature vector
assembler = VectorAssembler(inputCols=selected_features, outputCol="features")
final_data = assembler.transform(processed_data)

# Create binary classification
median_score = final_data.approxQuantile("current_score", [0.5], 0.01)[0]
final_data = final_data.withColumn("label",
    when(col("current_score") >= median_score, 1.0).otherwise(0.0))

# Split the data
train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)

# Initialize the models
rf = RandomForestClassifier(labelCol="label",
                           featuresCol="features",
                           numTrees=100,
                           seed=42)

gbt = GBTClassifier(labelCol="label",
                    featuresCol="features",
                    maxIter=100,
                    seed=42)

dt = DecisionTreeClassifier(labelCol="label",
                           featuresCol="features",
                           seed=42)

layers = [len(selected_features), 20, 10, 2]
mlp = MultilayerPerceptronClassifier(labelCol="label",
                                    featuresCol="features",
                                    layers=layers,
                                    seed=42)

# Dictionary to store models
models = {
    "Random Forest": rf,
    "Gradient Boosting Tree": gbt,
    "Decision Tree": dt,
    "Neural Network": mlp
}

# Initial model evaluation
evaluator_multi = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="prediction"
)
evaluator_binary = BinaryClassificationEvaluator(
    labelCol="label",
    rawPredictionCol="rawPrediction"
)

metrics_multi = ["accuracy", "weightedPrecision", "weightedRecall", "f1"]
results = {}

print("Initial Model Performance Results:")
print("-" * 50)

for name, model in models.items():
    print(f"\nTraining {name}...")

    # Train model
    fitted_model = model.fit(train_data)
    predictions = fitted_model.transform(test_data)

    # Calculate metrics
    model_metrics = {}

    # Multiclass metrics
    for metric in metrics_multi:
        evaluator_multi.setMetricName(metric)
        score = evaluator_multi.evaluate(predictions)
        model_metrics[metric] = score

    # AUC score
    auc_score = evaluator_binary.evaluate(predictions)
    model_metrics['auc'] = auc_score

    results[name] = model_metrics

    # Print results
    print(f"\n{name} Results:")
    for metric, score in model_metrics.items():
        print(f"{metric}: {score:.3f}")

# Select top 2 models for hyperparameter tuning
sorted_models = sorted(results.items(),
                      key=lambda x: x[1]['accuracy'],
                      reverse=True)[:2]

print("\nHyperparameter Tuning for Top 2 Models:")
print("-" * 50)

# Define evaluator for hyperparameter tuning
tuning_evaluator = BinaryClassificationEvaluator(
    labelCol="label",
    rawPredictionCol="rawPrediction",
    metricName="areaUnderROC"
)

for model_name, _ in sorted_models:
    print(f"\nTuning {model_name}...")

    if model_name == "Random Forest":
        base_model = RandomForestClassifier(labelCol="label",
                                          featuresCol="features",
                                          seed=42)
        paramGrid = ParamGridBuilder()\
            .addGrid(base_model.maxDepth, [5, 10])\
            .addGrid(base_model.numTrees, [50, 100])\
            .build()

    elif model_name == "Gradient Boosting Tree":
        base_model = GBTClassifier(labelCol="label",
                                 featuresCol="features",
                                 seed=42)
        paramGrid = ParamGridBuilder()\
            .addGrid(base_model.maxDepth, [3, 5])\
            .addGrid(base_model.stepSize, [0.1, 0.2])\
            .build()

    elif model_name == "Decision Tree":
        base_model = DecisionTreeClassifier(labelCol="label",
                                          featuresCol="features",
                                          seed=42)
        paramGrid = ParamGridBuilder()\
            .addGrid(base_model.maxDepth, [5, 10])\
            .addGrid(base_model.minInstancesPerNode, [1, 2])\
            .build()

    elif model_name == "Neural Network":
        base_model = MultilayerPerceptronClassifier(labelCol="label",
                                                  featuresCol="features",
                                                  seed=42)
        paramGrid = ParamGridBuilder()\
            .addGrid(base_model.layers, [[len(selected_features), 20, 2],
                                       [len(selected_features), 30, 2]])\
            .addGrid(base_model.maxIter, [100, 200])\
            .build()

    # Create CrossValidator
    crossval = CrossValidator(
        estimator=base_model,
        estimatorParamMaps=paramGrid,
        evaluator=tuning_evaluator,
        numFolds=3,
        seed=42
    )

    # Fit CrossValidator
    print(f"Training {model_name} with cross-validation...")
    cv_model = crossval.fit(train_data)

    # Get best model
    best_model = cv_model.bestModel

    # Make predictions with best model
    predictions = best_model.transform(test_data)

    # Calculate final metrics
    print(f"\nBest {model_name} Model Results:")
    for metric in metrics_multi:
        evaluator_multi.setMetricName(metric)
        score = evaluator_multi.evaluate(predictions)
        print(f"{metric}: {score:.3f}")

 # Print the best parameters for the model
print("\nBest Parameters:")
for param in base_model.params:
    try:
        # Try to get the parameter value
        value = best_model.getOrDefault(param)
        print(f"{param.name}: {value}")
    except KeyError:
        # Skip parameters that are not supported by the model
        continue

print("\nModel Training and Evaluation Completed!")